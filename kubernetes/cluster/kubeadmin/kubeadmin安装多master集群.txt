初始化安装k8s集群的实验环境
1.1 修改机器IP，变成静态IP
vim /etc/sysconfig/network-scripts/ifcfg-ens33文件
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=static
IPADDR=192.168.110.23
NETMASK=255.255.255.0
GATEWAY=192.168.110.1
DNS1=114.114.114.114
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=ens33
DEVICE=ens33
ONBOOT=yes

#修改配置文件之后需要重启网络服务才能使配置生效，重启网络服务命令如下：
systemctl status firewalld

注：/etc/sysconfig/network-scripts/ifcfg-ens33文件里的配置说明：
NAME=ens33    #网卡名字，跟DEVICE名字保持一致即可
DEVICE=ens33   #网卡设备名，大家ip addr可看到自己的这个网卡设备名，每个人的机器可能这个名字不一样，需要写自己的
BOOTPROTO=static   #static表示静态ip地址
ONBOOT=yes    #开机自启动网络，必须是yes
IPADDR=192.168.110.23   #ip地址，需要跟自己电脑所在网段一致
NETMASK=255.255.255.0  #子网掩码，需要跟自己电脑所在网段一致
GATEWAY=192.168.110.1   #网关，在自己电脑打开cmd，输入ipconfig /all可看到
DNS1=114.114.114.114    #DNS，在自己电脑打开cmd，输入ipconfig /all可看到 
    
1.2 配置机器主机名
在192.168.110.23上执行如下：
hostnamectl set-hostname kubeadmmaster1 && bash 
在192.168.110.24上执行如下：
hostnamectl set-hostname kubeadmmaster2 && bash
在192.168.110.25上执行如下：
hostnamectl set-hostname kubeadmmaster3 && bash
在192.168.110.26上执行如下：
hostnamectl set-hostname kubeadmnode1 && bash
在192.168.110.27上执行如下：
hostnamectl set-hostname kubeadmnode2 && bash

1.3 配置主机hosts文件，相互之间通过主机名互相访问
修改每台机器的/etc/hosts文件，增加如下三行：
192.168.110.23   kubeadmmaster1  
192.168.110.24   kubeadmmaster2  
192.168.110.25   kubeadmmaster3  
192.168.110.26   kubeadmnode1  
192.168.110.27   kubeadmnode2  

1.4 配置主机之间无密码登录
[root@kubeadmmaster1 ~]# ssh-keygen  #一路回车，不输入密码
把本地生成的密钥文件和私钥文件拷贝到远程主机
[root@kubeadmmaster1 ~]# ssh-copy-id kubeadmmaster1
[root@kubeadmmaster1 ~]# ssh-copy-id kubeadmmaster2
[root@kubeadmmaster1 ~]# ssh-copy-id kubeadmmaster3
[root@kubeadmmaster1 ~]# ssh-copy-id kubeadmnode1
[root@kubeadmmaster1 ~]# ssh-copy-id kubeadmnode2

在kubeadmmaster1、kubeadmmaster2、kubeadmmaster3、kubeadmnode1、kubeadmnode2上重复执行上面命令


1.5 关闭交换分区swap，提升性能
#临时关闭
[root@kubeadmmaster1 ~]# swapoff -a

#永久关闭：注释swap挂载，给swap这行开头加一下注释
[root@kubeadmmaster1 ~]# vim /etc/fstab   
# /dev/mapper/centos-swap swap      swap    defaults        0 0

在kubeadmmaster1、kubeadmmaster2、kubeadmmaster3、kubeadmnode1、kubeadmnode2上重复执行上面命令

问题1：为什么要关闭swap交换分区？
Swap是交换分区，如果机器内存不够，会使用swap分区，但是swap分区的性能较低，k8s设计的时候为了能提升性能，默认是不允许使用交换分区的。Kubeadm初始化的时候会检测swap是否关闭，如果没关闭，那就初始化失败。如果不想要关闭交换分区，安装k8s的时候可以指定--ignore-preflight-errors=Swap来解决。

1.6 修改机器内核参数 
[root@kubeadmmaster1 ~]# modprobe br_netfilter
[root@kubeadmmaster1 ~]# echo "modprobe br_netfilter" >> /etc/profile
[root@kubeadmmaster1 ~]# cat > /etc/sysctl.d/k8s.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
[root@kubeadmmaster1 ~]# sysctl -p /etc/sysctl.d/k8s.conf

在kubeadmmaster1、kubeadmmaster2、kubeadmmaster3、kubeadmnode1、kubeadmnode2上重复执行上面命令


问题1：sysctl是做什么的？
在运行时配置内核参数
  -p   从指定的文件加载系统参数，如不指定即从/etc/sysctl.conf中加载

问题2：为什么要执行modprobe br_netfilter？
修改/etc/sysctl.d/k8s.conf文件，增加如下三行参数：
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1

sysctl -p /etc/sysctl.d/k8s.conf出现报错：

sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-ip6tables: No such file or directory
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory

解决方法：
modprobe br_netfilter

问题3：为什么开启net.bridge.bridge-nf-call-iptables内核参数？
在centos下安装docker，执行docker info出现如下警告：
WARNING: bridge-nf-call-iptables is disabled
WARNING: bridge-nf-call-ip6tables is disabled

解决办法：
vim  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

问题4：为什么要开启net.ipv4.ip_forward = 1参数？
kubeadm初始化k8s如果报错：
 
就表示没有开启ip_forward，需要开启。

net.ipv4.ip_forward是数据包转发：
出于安全考虑，Linux系统默认是禁止数据包转发的。所谓转发即当主机拥有多于一块的网卡时，其中一块收到数据包，根据数据包的目的ip地址将数据包发往本机另一块网卡，该网卡根据路由表继续发送数据包。这通常是路由器所要实现的功能。
要让Linux系统具有路由转发功能，需要配置一个Linux的内核参数net.ipv4.ip_forward。这个参数指定了Linux系统当前对路由转发功能的支持情况；其值为0时表示禁止进行IP转发；如果是1,则说明IP转发功能已经打开。


1.7 关闭firewalld防火墙
[root@kubeadmmaster1 ~]# systemctl stop firewalld ; systemctl disable firewalld

在kubeadmmaster1、kubeadmmaster2、kubeadmmaster3、kubeadmnode1、kubeadmnode2上重复执行上面命令


1.7 关闭selinux
[root@kubeadmmaster1 ~]# sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
#修改selinux配置文件之后，重启机器，selinux配置才能永久生效

1.8 配置阿里云的repo源
 
安装rzsz命令
[root@kubeadmmaster1]# yum install lrzsz -y
安装scp：
[root@kubeadmmaster1]#yum install openssh-clients

#备份基础repo源
[root@kubeadmmaster1 ~]# mkdir /root/repo.bak
[root@kubeadmmaster1 ~]# cd /etc/yum.repos.d/
[root@kubeadmmaster1]# mv * /root/repo.bak/
#下载阿里云的repo源
把CentOS-Base.repo文件上传到kubeadmmaster1主机的/etc/yum.repos.d/目录下

#配置国内阿里云docker的repo源
[root@kubeadmmaster1 ~]# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

在kubeadmmaster1、kubeadmmaster2、kubeadmmaster3、kubeadmnode1、kubeadmnode2上重复执行上面命令

1.9 配置安装k8s组件需要的阿里云的repo源 
[root@kubeadmmaster1 ~]#vim  /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0

#将kubeadmmaster1上Kubernetes的repo源复制给kubeadmmaster2、kubeadmmaster3、kubeadmnode1、kubeadmnode2

[root@kubeadmmaster1 ~]# scp /etc/yum.repos.d/kubernetes.repo kubeadmmaster2:/etc/yum.repos.d/
[root@kubeadmmaster1 ~]# scp /etc/yum.repos.d/kubernetes.repo kubeadmmaster3:/etc/yum.repos.d/
[root@kubeadmmaster1 ~]# scp /etc/yum.repos.d/kubernetes.repo kubeadmnode1:/etc/yum.repos.d/
[root@kubeadmmaster1 ~]# scp /etc/yum.repos.d/kubernetes.repo kubeadmnode2:/etc/yum.repos.d/

1.10 配置时间同步
#安装ntpdate命令
[root@kubeadmmaster1 ~]# yum install ntpdate -y
#跟网络时间做同步
[root@kubeadmmaster1 ~]# ntpdate cn.pool.ntp.org
#把时间同步做成计划任务
[root@kubeadmmaster1 ~]# crontab -e
* */1 * * * /usr/sbin/ntpdate   cn.pool.ntp.org
#重启crond服务
[root@kubeadmmaster1 ~]#service crond restart

在kubeadmmaster1、kubeadmmaster2、kubeadmmaster3、kubeadmnode1、kubeadmnode2上重复执行上面命令

1.11开启ipvs

在kubeadmmaster1、kubeadmmaster2、kubeadmmaster3、kubeadmnode1、kubeadmnode2上重复执行上面命令

1.12 安装基础软件包
[root@kubeadmmaster1 ~]# yum install -y yum-utils device-mapper-persistent-data lvm2 wget net-tools nfs-utils lrzsz gcc gcc-c++ make cmake libxml2-devel openssl-devel curl curl-devel unzip sudo ntp libaio-devel wget vim ncurses-devel autoconf automake zlib-devel  python-devel epel-release openssh-server socat  ipvsadm conntrack ntpdate telnet ipvsadm

在kubeadmmaster1、kubeadmmaster2、kubeadmmaster3、kubeadmnode1、kubeadmnode2上重复执行上面命令

1.13 安装iptables
如果用firewalld不习惯，可以安装iptables ，在kubeadmmaster1、kubeadmmaster2、kubeadmnode1上操作：
#安装iptables
yum install iptables-services -y
#禁用iptables
service iptables stop   && systemctl disable iptables
#清空防火墙规则
iptables -F

在kubeadmmaster1、kubeadmmaster2、kubeadmmaster3、kubeadmnode1、kubeadmnode2上重复执行上面命令

2、安装docker服务
2.1 安装docker-ce
[root@kubeadmmaster1 ~]# yum install docker-ce-20.10.6 docker-ce-cli-20.10.6 containerd.io  -y
[root@kubeadmmaster1 ~]# systemctl start docker && systemctl enable docker && systemctl status docker

在kubeadmmaster1、kubeadmmaster2、kubeadmmaster3、kubeadmnode1、kubeadmnode2上重复执行上面命令

2.2 配置docker镜像加速器和驱动
[root@kubeadmmaster1 ~]#vim  /etc/docker/daemon.json 
{
 "registry-mirrors":["https://rsbud4vc.mirror.aliyuncs.com","https://registry.docker-cn.com","https://docker.mirrors.ustc.edu.cn","https://dockerhub.azk8s.cn","http://hub-mirror.c.163.com","http://qtid6917.mirror.aliyuncs.com", "https://rncxm540.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"]
} 

#修改docker文件驱动为systemd，默认为cgroupfs，kubelet默认使用systemd，两者必须一致才可以。

[root@kubeadmmaster1 ~]# systemctl daemon-reload  && systemctl restart docker
[root@kubeadmmaster1 ~]# systemctl status docker

在kubeadmmaster1、kubeadmmaster2、kubeadmmaster3、kubeadmnode1、kubeadmnode2上重复执行上面命令

3、安装初始化k8s需要的软件包
[root@kubeadmmaster1 ~]# yum install -y kubelet-1.20.6 kubeadm-1.20.6 kubectl-1.20.6
[root@kubeadmmaster1 ~]# systemctl enable kubelet && systemctl start kubelet
[root@kubeadmmaster1 ~]# systemctl status kubelet
 
#上面可以看到kubelet状态不是running状态，这个是正常的，不用管，等k8s组件起来这个kubelet就正常了。


注：每个软件包的作用
Kubeadm:  kubeadm是一个工具，用来初始化k8s集群的
kubelet:   安装在集群所有节点上，用于启动Pod的
kubectl:   通过kubectl可以部署和管理应用，查看各种资源，创建、删除和更新各种组件

在kubeadmmaster1、kubeadmmaster2、kubeadmmaster3、kubeadmnode1、kubeadmnode2上重复执行上面命令
 
4、通过keepalive+nginx实现k8s apiserver节点高可用

#配置epel源
把epel.repo上传到kubeadmmaster1的/etc/yum.repos.d目录下，这样才能安装keepalived和nginx
#把epel.repo拷贝到远程主机kubeadmmaster2和kubeadmnode1上
[root@kubeadmmaster1 ~]# scp /etc/yum.repos.d/epel.repo kubeadmmaster2:/etc/yum.repos.d/
[root@kubeadmmaster1 ~]# scp /etc/yum.repos.d/epel.repo kubeadmmaster3:/etc/yum.repos.d/
[root@kubeadmmaster1 ~]# scp /etc/yum.repos.d/epel.repo kubeadmnode1:/etc/yum.repos.d/
[root@kubeadmmaster1 ~]# scp /etc/yum.repos.d/epel.repo kubeadmnode2:/etc/yum.repos.d/

1、安装nginx主备：
在kubeadmmaster1和kubeadmmaster2上做nginx主备安装

[root@kubeadmmaster1 ~]#  yum install nginx keepalived -y
[root@kubeadmmaster2 ~]#  yum install nginx keepalived -y

2、修改nginx配置文件。主备一样
[root@kubeadmmaster1 ~]# vim /etc/nginx/nginx.conf

user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log;
pid /run/nginx.pid;

include /usr/share/nginx/modules/*.conf;

events {
    worker_connections 1024;
}

# 四层负载均衡，为两台Master apiserver组件提供负载均衡
stream {
    log_format  main  '$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent';
    access_log  /var/log/nginx/k8s-access.log  main;
    upstream k8s-apiserver {
       server 192.168.110.23:6443;   # Master1 APISERVER IP:PORT
       server 192.168.110.24:6443;   # Master2 APISERVER IP:PORT
       server 192.168.110.25:6443;   # Master3 APISERVER IP:PORT
    }  
    server {
       listen 16443; # 由于nginx与master节点复用，这个监听端口不能是6443，否则会冲突
       proxy_pass k8s-apiserver;
    }
}

http {
    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    sendfile            on;
    tcp_nopush          on;
    tcp_nodelay         on;
    keepalive_timeout   65;
    types_hash_max_size 2048;

    include             /etc/nginx/mime.types;
    default_type        application/octet-stream;

    server {
        listen       80 default_server;
        server_name  _;

        location / {
        }
    }
}

在kubeadmmaster1、kubeadmmaster2


3、keepalive配置
主keepalived
[root@kubeadmmaster1 ~]# vim  /etc/keepalived/keepalived.conf 

global_defs {
   notification_email {
     acassen@firewall.loc
     failover@firewall.loc
     sysadmin@firewall.loc
   }
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 127.0.0.1
   smtp_connect_timeout 30
   router_id NGINX_MASTER
}

vrrp_script check_nginx {
    script "/etc/keepalived/check_nginx.sh"
}

vrrp_instance VI_1 {
    state MASTER
    interface ens192  # 修改为实际网卡名
    virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的
    priority 100    # 优先级，备服务器设置 90
    advert_int 1    # 指定VRRP 心跳包通告间隔时间，默认1秒
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    # 虚拟IP
    virtual_ipaddress {
        192.168.110.29/24
    }
    track_script {
        check_nginx
    }
}

#vrrp_script：指定检查nginx工作状态脚本（根据nginx状态判断是否故障转移）
#virtual_ipaddress：虚拟IP（VIP）

[root@kubeadmmaster1 ~]# vim  /etc/keepalived/check_nginx.sh 

#!/bin/bash
#1、判断Nginx是否存活
counter=`ps -C nginx --no-header | wc -l`
if [ $counter -eq 0 ]; then
    #2、如果不存活则尝试启动Nginx
    service nginx start
    sleep 2
    #3、等待2秒后再次获取一次Nginx状态
    counter=`ps -C nginx --no-header | wc -l`
    #4、再次进行判断，如Nginx还不存活则停止Keepalived，让地址进行漂移
    if [ $counter -eq 0 ]; then
        service  keepalived stop
    fi
fi

[root@kubeadmmaster1 ~]# chmod +x /etc/keepalived/check_nginx.sh

备keepalive
[root@kubeadmmaster2 ~]# vim  /etc/keepalived/keepalived.conf 

global_defs {
   notification_email {
     acassen@firewall.loc
     failover@firewall.loc
     sysadmin@firewall.loc
   }
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 127.0.0.1
   smtp_connect_timeout 30
   router_id NGINX_BACKUP
}

vrrp_script check_nginx {
    script "/etc/keepalived/check_nginx.sh"
}

vrrp_instance VI_1 {
    state BACKUP
    interface ens192  # 修改为实际网卡名
    virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的
    priority 90    # 优先级，备服务器设置 90
    advert_int 1    # 指定VRRP 心跳包通告间隔时间，默认1秒
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    # 虚拟IP
    virtual_ipaddress {
        192.168.110.29/24
    }
    track_script {
        check_nginx
    }
}


[root@kubeadmmaster2 ~]# vim  /etc/keepalived/check_nginx.sh 
#!/bin/bash
#1、判断Nginx是否存活
counter=`ps -C nginx --no-header | wc -l`
if [ $counter -eq 0 ]; then
    #2、如果不存活则尝试启动Nginx
    service nginx start
    sleep 2
    #3、等待2秒后再次获取一次Nginx状态
    counter=`ps -C nginx --no-header | wc -l`
    #4、再次进行判断，如Nginx还不存活则停止Keepalived，让地址进行漂移
    if [ $counter -eq 0 ]; then
        service  keepalived stop
    fi
fi

[root@kubeadmmaster2 ~]# chmod +x /etc/keepalived/check_nginx.sh
#注：keepalived根据脚本返回状态码（0为工作正常，非0不正常）判断是否故障转移。

4、启动服务：
[root@kubeadmmaster1 ~]# systemctl daemon-reload
[root@kubeadmmaster1 ~]# systemctl start nginx
[root@kubeadmmaster1 ~]# systemctl start keepalived
[root@kubeadmmaster1 ~]# systemctl enable nginx keepalived
[root@kubeadmmaster1]# systemctl status keepalived
 
在kubeadmmaster1、kubeadmmaster2
 

5、测试vip是否绑定成功
[root@kubeadmmaster1 ~]# ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 00:0c:29:79:9e:36 brd ff:ff:ff:ff:ff:ff
    inet 192.168.40.180/24 brd 192.168.40.255 scope global noprefixroute ens33
       valid_lft forever preferred_lft forever
    inet 192.168.40.199/24 scope global secondary ens33
       valid_lft forever preferred_lft forever
    inet6 fe80::b6ef:8646:1cfc:3e0c/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever


6、测试keepalived：
停掉kubeadmmaster1上的nginx。Vip会漂移到kubeadmmaster2
[root@kubeadmmaster1 ~]# service nginx stop
[root@kubeadmmaster2]# ip addr
 


#启动kubeadmmaster1上的nginx和keepalived，vip又会漂移回来
[root@kubeadmmaster1 ~]# systemctl daemon-reload
[root@kubeadmmaster1 ~]# systemctl start nginx
[root@kubeadmmaster1 ~]# systemctl start keepalived
[root@kubeadmmaster1]# ip addr
 


5、kubeadm初始化k8s集群

在kubeadmmaster1上创建kubeadm-config.yaml文件：
[root@kubeadmmaster1 ~]# cd /root/
[root@kubeadmmaster1]# vim kubeadm-config.yaml 
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.20.6
controlPlaneEndpoint: 192.168.40.199:16443
imageRepository: registry.aliyuncs.com/google_containers
apiServer:
 certSANs:
 - 192.168.110.23
 - 192.168.110.24
 - 192.168.110.25
 - 192.168.110.26
 - 192.168.110.27
 - 192.168.110.29
networking:
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.10.0.0/16
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind:  KubeProxyConfiguration
mode: ipvs

#使用kubeadm初始化k8s集群
#把初始化k8s集群需要的离线镜像包上传到kubeadmmaster1、kubeadmmaster2、kubeadmmaster3、kubeadmnode1、kubeadmnode2机器上，手动解压：
[root@kubeadmmaster1 ~]# docker load -i k8simage-1-20-6.tar.gz
[root@kubeadmmaster1]# kubeadm init --config kubeadm-config.yaml

注：--image-repository registry.aliyuncs.com/google_containers：手动指定仓库地址为registry.aliyuncs.com/google_containers。kubeadm默认从k8s.grc.io拉取镜像，但是k8s.gcr.io访问不到，所以需要指定从registry.aliyuncs.com/google_containers仓库拉取镜像。

显示如下，说明安装完成：
 


  kubeadm join 192.168.110.29:16443 --token zwzcks.u4jd8lj56wpckcwv \
    --discovery-token-ca-cert-hash sha256:1ba1b274090feecfef58eddc2a6f45590299c1d0624618f1f429b18a064cb728 \
    --control-plane
    #上面命令是把master节点加入集群，需要保存下来，每个人的都不一样
  kubeadm join 192.168.110.29:16443 --token zwzcks.u4jd8lj56wpckcwv \
    --discovery-token-ca-cert-hash sha256:1ba1b274090feecfef58eddc2a6f45590299c1d0624618f1f429b18a064cb728
   #上面命令是把node节点加入集群，需要保存下来，每个人的都不一样
   
#配置kubectl的配置文件config，相当于对kubectl进行授权，这样kubectl命令可以使用这个证书对k8s集群进行管理
[root@kubeadmmaster1 ~]# mkdir -p $HOME/.kube
[root@kubeadmmaster1 ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@kubeadmmaster1 ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config
[root@kubeadmmaster1 ~]# kubectl get nodes
NAME               STATUS    ROLES                  AGE   VERSION
kubeadmmaster1   NotReady   control-plane,master    60s   v1.20.6
此时集群状态还是NotReady状态，因为没有安装网络插件。  
6、扩容k8s集群-添加master节点
#把kubeadmmaster1节点的证书拷贝到kubeadmmaster2上
在kubeadmmaster2创建证书存放目录：
[root@kubeadmmaster2 ~]# cd /root && mkdir -p /etc/kubernetes/pki/etcd &&mkdir -p ~/.kube/
#把kubeadmmaster1节点的证书拷贝到kubeadmmaster2上：
[root@kubeadmmaster1 ~]# scp /etc/kubernetes/pki/ca.crt kubeadmmaster2:/etc/kubernetes/pki/
[root@kubeadmmaster1 ~]# scp /etc/kubernetes/pki/ca.key kubeadmmaster2:/etc/kubernetes/pki/
[root@kubeadmmaster1 ~]# scp /etc/kubernetes/pki/sa.key kubeadmmaster2:/etc/kubernetes/pki/
[root@kubeadmmaster1 ~]# scp /etc/kubernetes/pki/sa.pub kubeadmmaster2:/etc/kubernetes/pki/
[root@kubeadmmaster1 ~]# scp /etc/kubernetes/pki/front-proxy-ca.crt kubeadmmaster2:/etc/kubernetes/pki/
[root@kubeadmmaster1 ~]# scp /etc/kubernetes/pki/front-proxy-ca.key kubeadmmaster2:/etc/kubernetes/pki/
[root@kubeadmmaster1 ~]# scp /etc/kubernetes/pki/etcd/ca.crt kubeadmmaster2:/etc/kubernetes/pki/etcd/
[root@kubeadmmaster1 ~]# scp /etc/kubernetes/pki/etcd/ca.key kubeadmmaster2:/etc/kubernetes/pki/etcd/


#证书拷贝之后在kubeadmmaster2上执行如下命令，大家复制自己的，这样就可以把kubeadmmaster2和加入到集群，成为控制节点：

在kubeadmmaster1上查看加入节点的命令：
[root@kubeadmmaster1 ~]# kubeadm token create --print-join-command

kubeadm join 192.168.40.199:16443 --token zwzcks.u4jd8lj56wpckcwv \
    --discovery-token-ca-cert-hash sha256:1ba1b274090feecfef58eddc2a6f45590299c1d0624618f1f429b18a064cb728 \
    --control-plane 

 
#看到上面说明kubeadmmaster2节点已经加入到集群了


在kubeadmmaster1上查看集群状况：
[root@kubeadmmaster1 ~]#  kubectl get nodes
NAME              STATUS     ROLES                  AGE   VERSION
kubeadmmaster1   NotReady   control-plane,master   49m   v1.20.6
kubeadmmaster2   NotReady   <none>                 39s   v1.20.6
     
上面可以看到kubeadmmaster2已经加入到集群了


7、扩容k8s集群-添加node节点
在kubeadmmaster1上查看加入节点的命令：
[root@kubeadmmaster1 ~]# kubeadm token create --print-join-command
#显示如下：
kubeadm join 192.168.40.199:16443 --token y23a82.hurmcpzedblv34q8     --discovery-token-ca-cert-hash sha256:1ba1b274090feecfef58eddc2a6f45590299c1d0624618f1f429b18a064cb728
把kubeadmnode1加入k8s集群：
[root@kubeadmnode1~]# kubeadm token create --print-join-command
kubeadm join 192.168.40.199:16443 --token y23a82.hurmcpzedblv34q8     --discovery-token-ca-cert-hash sha256:1ba1b274090feecfef58eddc2a6f45590299c1d0624618f1f429b18a064cb728

 

#看到上面说明kubeadmnode1节点已经加入到集群了,充当工作节点

#在kubeadmmaster1上查看集群节点状况：
[root@kubeadmmaster1 ~]# kubectl get nodes
NAME              STATUS     ROLES                  AGE     VERSION
kubeadmmaster1   NotReady   control-plane,master   53m     v1.20.6
kubeadmmaster2   NotReady   control-plane,master   5m13s   v1.20.6
kubeadmnode1     NotReady   <none>                 59s     v1.20.6

#可以看到kubeadmnode1的ROLES角色为空，<none>就表示这个节点是工作节点。
#可以把kubeadmnode1的ROLES变成work，按照如下方法：
[root@kubeadmmaster1 ~]# kubectl label node kubeadmnode1 node-role.kubernetes.io/worker=worker

注意：上面状态都是NotReady状态，说明没有安装网络插件

[root@kubeadmmaster1 ~]# kubectl get pods -n kube-system
NAME                                      READY   STATUS    RESTARTS   AGE
coredns-7f89b7bc75-lh28j                  0/1     Pending   0          18h
coredns-7f89b7bc75-p7nhj                  0/1     Pending   0          18h
etcd-kubeadmmaster1                      1/1     Running   0          18h
etcd-kubeadmmaster2                      1/1     Running   0          15m
kube-apiserver-kubeadmmaster1            1/1     Running   0          18h
kube-apiserver-kubeadmmaster2            1/1     Running   0          15m
kube-controller-manager-kubeadmmaster1   1/1     Running   1          18h
kube-controller-manager-kubeadmmaster2   1/1     Running   0          15m
kube-proxy-n26mf                          1/1     Running   0          4m33s
kube-proxy-sddbv                          1/1     Running   0          18h
kube-proxy-sgqm2                          1/1     Running   0          15m
kube-scheduler-kubeadmmaster1            1/1     Running   1          18h
kube-scheduler-kubeadmmaster2            1/1     Running   0          15m

coredns-7f89b7bc75-lh28j是pending状态，这是因为还没有安装网络插件，等到下面安装好网络插件之后这个cordns就会变成running了


8、安装kubernetes网络组件-Calico
上传calico.yaml到kubeadmmaster1上，使用yaml文件安装calico 网络插件 。
[root@kubeadmmaster1 ~]# kubectl apply -f  calico.yaml

注：在线下载配置文件地址是： https://docs.projectcalico.org/manifests/calico.yaml
。
[root@kubeadmmaster1 ~]# kubectl get pod -n kube-system 
 

coredns-这个pod现在是running状态，运行正常

再次查看集群状态。
[root@kubeadmmaster1 ~]# kubectl get nodes
NAME              STATUS   ROLES                  AGE     VERSION
kubeadmmaster1   Ready    control-plane,master   58m     v1.20.6
kubeadmmaster2   Ready    control-plane,master   10m     v1.20.6
kubeadmnode1     Ready    <none>                 5m46s   v1.20.6

#STATUS状态是Ready，说明k8s集群正常运行了


9、测试在k8s创建pod是否可以正常访问网络
#把busybox-1-28.tar.gz上传到kubeadmnode1节点，手动解压
[root@kubeadmnode1 ~]# docker load -i busybox-1-28.tar.gz
[root@kubeadmmaster1 ~]# kubectl run busybox --image busybox:1.28 --restart=Never --rm -it busybox -- sh
/ # ping www.baidu.com
PING www.baidu.com (39.156.66.18): 56 data bytes
64 bytes from 39.156.66.18: seq=0 ttl=127 time=39.3 ms
#通过上面可以看到能访问网络，说明calico网络插件已经被正常安装了

10、测试k8s集群中部署tomcat服务
#把tomcat.tar.gz上传到kubeadmnode1，手动解压
[root@kubeadmnode1 ~]# docker load -i tomcat.tar.gz
[root@kubeadmmaster1 ~]# kubectl apply -f tomcat.yaml
[root@kubeadmmaster1 ~]#  kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
demo-pod   1/1     Running   0          10s
[root@kubeadmmaster1 ~]# kubectl apply -f tomcat-service.yaml
[root@kubeadmmaster1 ~]# kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP   10.255.0.1       <none>        443/TCP          158m
tomcat       NodePort    10.255.227.179   <none>        8080:30080/TCP   19m

在浏览器访问kubeadmnode1节点的ip:30080即可请求到浏览器
 

11、测试coredns是否正常
[root@kubeadmmaster1 ~]# kubectl run busybox --image busybox:1.28 --restart=Never --rm -it busybox -- sh
If you don't see a command prompt, try pressing enter.
/ # nslookup kubernetes.default.svc.cluster.local
Server:    10.10.0.10
Address 1: 10.10.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default.svc.cluster.local
Address 1: 10.10.0.1 kubernetes.default.svc.cluster.local
/ # nslookup tomcat.default.svc.cluster.local
Server:    10.10.0.10
Address 1: 10.10.0.10 kube-dns.kube-system.svc.cluster.local

Name:      tomcat.default.svc.cluster.local
Address 1: 10.10.13.88 tomcat.default.svc.cluster.local

10.10.13.88就是我们coreDNS的clusterIP，说明coreDNS配置好了。
解析内部Service的名称，是通过coreDNS去解析的。

10.10.0.10是创建的tomcat的service ip

#注意：
busybox要用指定的1.28版本
